# 🧠 Sentence-Level GPT from Scratch

A lightweight, custom-built GPT-like Transformer language model trained from scratch on 85,000 English sentences.  
Utilizes a BERT tokenizer and includes full training, inference etc in a single Jupyter Notebook.

---

## 📁 Project Structure

This implementation is currently fully contained within a Jupyter Notebook for clarity and ease of experimentation.  
All steps — from tokenization and model architecture to training and evaluation — are accessible and well-documented.

Python scripts (`model.py`, `train.py`, etc.) have been initialized to support modularization.  
These files will be filled in future updates to transition toward a clean, production-ready codebase.

> 🔧 For now, please refer to the notebook for the complete working implementation.

---

## 🚀 Features

- ✅ Built-from-scratch GPT-style model (decoder-only Transformer)
- 🧠 3.5M parameters
- 📚 Trained on 85,000 natural English sentences
- 🔤 BERT tokenizer for subword-level vocabulary
- 🗂️ Modular Python scripts planned

---
## 🗂️ File Overview

| File / Folder              | Description                                           |
|----------------------------|-------------------------------------------------------|
| `gpt_sentencemodel.ipynb`  | Complete implementation and training flow             |
| `model.py`                 | *(To be added)* Model architecture definition         |
| `train.py`                 | *(To be added)* Training loop and validation logic    |
| `utils.py`                 | *(To be added)* Tokenization, dataset prep, etc.      |
