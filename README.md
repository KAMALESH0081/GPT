# ğŸ§  Sentence-Level GPT from Scratch

A lightweight, custom-built GPT-like Transformer language model trained from scratch on 85,000 English sentences.  
Utilizes a BERT tokenizer and includes full training, inference etc in a single Jupyter Notebook.

---

## ğŸ“ Project Structure

This implementation is currently fully contained within a Jupyter Notebook for clarity and ease of experimentation.  
All steps â€” from tokenization and model architecture to training and evaluation â€” are accessible and well-documented.

Python scripts (`model.py`, `train.py`, etc.) have been initialized to support modularization.  
These files will be filled in future updates to transition toward a clean, production-ready codebase.

> ğŸ”§ For now, please refer to the notebook for the complete working implementation.

---

## ğŸš€ Features

- âœ… Built-from-scratch GPT-style model (decoder-only Transformer)
- ğŸ§  3.5M parameters
- ğŸ“š Trained on 85,000 natural English sentences
- ğŸ”¤ BERT tokenizer for subword-level vocabulary
- ğŸ—‚ï¸ Modular Python scripts planned

---
## ğŸ—‚ï¸ File Overview

| File / Folder              | Description                                           |
|----------------------------|-------------------------------------------------------|
| `gpt_sentencemodel.ipynb`  | Complete implementation and training flow             |
| `model.py`                 | *(To be added)* Model architecture definition         |
| `train.py`                 | *(To be added)* Training loop and validation logic    |
| `utils.py`                 | *(To be added)* Tokenization, dataset prep, etc.      |
