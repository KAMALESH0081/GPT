#  Sentence-Level GPT from Scratch

A lightweight, custom-built GPT-like Transformer language model trained from scratch on 85,000 English sentences.  
Utilizes a BERT tokenizer and includes full training, inference, and evaluation in a single Jupyter Notebook.

---

## ğŸ“ Project Structure

This implementation is currently fully contained within a Jupyter Notebook for clarity and ease of experimentation.  
All steps â€” from tokenization and model architecture to training and evaluation â€” are accessible and well-documented.

Python scripts (`model.py`, `train.py`, etc.) have been initialized to support modularization.  
These files will be filled in future updates to transition toward a clean, production-ready codebase.

> ğŸ”§ For now, please refer to the notebook for the complete working implementation.

---

##  Features

- âœ… Built-from-scratch GPT-style model (decoder-only Transformer)
- ğŸ§  ~3.5 million parameters
- ğŸ“š Trained on 85,000 natural English sentences
- ğŸ”¤ BERT tokenizer for subword-level vocabulary
- ğŸ—‚ï¸ Modular Python scripts planned (`model.py`, `train.py`, etc.)
- ğŸ–¥ï¸ Gradio-based web UI will be added for simple and interactive inference

---

